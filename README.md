# Trabajo Pr√°ctico Final ‚Äî Introducci√≥n al Marketing Online y los Negocios Digitales

Repositorio del trabajo pr√°ctico final de la materia.

**Consigna y documento principal:** [Trabajo Pr√°ctico Final](https://docs.google.com/document/d/15RNP3FVqLjO4jzh80AAkK6mUR5DOLqPxLjQxqvdzrYg/edit?usp=sharing)
**Diagrama Entidad Relaci√≥n:** [DER](./assets/DER.png)


Este proyecto implementa un pipeline **ETL completo** para la materia *Introducci√≥n al Marketing y los Negocios Digitales*, utilizando datos crudos (`raw/*.csv`) y generando un **Data Warehouse** con tablas de dimensiones y hechos en la carpeta `warehouse/`.

## Arquitectura del Proyecto
El proyecto sigue una estructura ETL cl√°sica:

1. raw/: Guarda los 13 archivos .CSV originales que simulan la base de datos transaccional (OLTP) de la empresa.
2. etl/: Contiene toda la l√≥gica de transformaci√≥n, separada en:
 - etl/extract/: Scripts para leer los datos desde data/raw/.
 - etl/transform/: Scripts para limpiar, denormalizar y construir cada tabla de Dimensi√≥n y Hechos.
 - etl/load/: Scripts para guardar los dataframes transformados en el directorio warehouse/.
3. warehouse/: Aqui dentro se encuentran las dim y facts desnormalizadas
 - warehouse/dim/: Contiene las tablas de dimensiones desnormalizadas
 - warehouse/fact/: Contiene las tablas de hechos desnormalizadas
4. main.py: El script orquestador que llama a las funciones de extract, transform y load en el orden correcto.

## üöÄ Ejecuci√≥n

### 1. Clonar el repositorio
```bash
git clone < https://github.com/Facurubiolo/mkt_tp_final >
cd mkt_tp_final
```

### 2. Crear y Activar Entorno Virtual
``` bash
python -m venv .venv
source .venv/Scripts/activate  
```

### 3. Instalar dependencias
``` bash
pip install -r requirements.txt
```

### 4. Ejecutar el pipeline ETL
``` bash
python main.py
```

## üß± 2. Supuestos y Dise√±o del Modelo

### üîπ Dise√±o general

El modelo del Data Warehouse se construy√≥ siguiendo una **arquitectura en estrella (Star Schema)**, conformada por:

- **6 tablas de dimensiones (DIM)**
- **6 tablas de hechos (FACT)**

Cada tabla de dimensi√≥n contiene una **surrogate key (clave artificial incremental)** que se utiliza como **clave primaria (PK)**.  
Las tablas de hechos utilizan dichas claves como **claves for√°neas (FK)** para permitir la integraci√≥n y an√°lisis cruzado.

---

### üîπ Supuestos principales del dise√±o

1. **Integridad referencial**:  
   Todas las claves en las tablas de hechos (`customer_id`, `channel_id`, `store_id`, etc.) se relacionan con las claves de las tablas de dimensiones correspondientes.

2. **Surrogate Keys (claves sustitutas):**  
   Cada tabla de dimensi√≥n genera una clave incremental (`dim_xxx_sk`) para evitar dependencias de claves de negocio originales.

3. **Fechas normalizadas:**  
   La dimensi√≥n `dim_calendar` fue construida para reemplazar fechas en las tablas de hechos con `date_id` en formato `YYYYMMDD`.

4. **Desnormalizaci√≥n controlada:**  
   Se combinaron campos de distintas tablas `raw/` (por ejemplo, `store` + `address` + `province`) para obtener una vista unificada y optimizada para an√°lisis.

5. **Campos monetarios:**  
   Los valores num√©ricos (`subtotal`, `tax_amount`, `shipping_fee`, `total_amount`) fueron convertidos a tipo `float`, reemplazando comas por puntos en caso de ser necesario.

6. **Fuentes de datos:**  
   Todos los datos provienen de archivos CSV ubicados en la carpeta `raw/`, los cuales representan distintas entidades del negocio de e-commerce.

7. **Pipeline reproducible:**  
   El proceso ETL completo puede ejecutarse de forma autom√°tica mediante `python main.py`, generando los resultados en `warehouse/`.

---

## üìò 3. Diccionario de Datos
El Data Warehouse est√° compuesto por **6 dimensiones** y **6 tablas de hechos**.  
Cada tabla de dimensi√≥n incluye una *surrogate key (SK)* como PK, mientras que las tablas de hechos contienen las FK necesarias para el an√°lisis en esquema estrella.

| Tipo | Tablas |
|------|---------|
| **Dimensiones** | dim_customer, dim_address, dim_product, dim_store, dim_channel, dim_calendar |
| **Hechos** | fact_sales_order, fact_sales_order_item, fact_payment, fact_shipment, fact_web_session, fact_nps_response |

### üìé Enlaces a los Esquemas Estrella0

Cada hecho tiene su propio esquema estrella, dise√±ado con [dbdiagram.io](https://dbdiagram.io).  

| Hecho | Enlace al esquema |
|-------|-------------------|
| **fact_sales_order** | [üìä Star Schema - Ventas](assets/star_sales_order.png) |
| **fact_sales_order_item** | [üìä Star Schema - √çtems de Venta](assets/star_sales_order_item.png) |
| **fact_payment** | [üìä Star Schema - Pagos](assets/star_payment.png) |
| **fact_shipment** | [üìä Star Schema - Env√≠os](assets/star_shipment.png) |
| **fact_web_session** | [üìä Star Schema - Sesiones Web](assets/star_web_session.png) |
| **fact_nps_response** | [üìä Star Schema - NPS](assets/star_nps_response.png) |


## 4. Consultas CLave 

Para poder calcular los KPIs pedidos y utilizados en las visualizaciones, se crearon medidas DAX en la tabla Medidas en el modelo de Power Bi

### Ticket Promedio
```dax
Ticket Promedio ($K) = 
CALCULATE(
    SUM(fact_sales_order[total_amount]) /
    DISTINCTCOUNT(fact_sales_order[id]),
    fact_sales_order[status_order] IN {"PAID", "FULFILLED"}
)
```

### NPS
```dax
NPS = 
VAR Promoters =
    COUNTROWS(
        FILTER(
            fact_nps_response,
            fact_nps_response[score] >= 9
        )
    )
VAR Detractors =
    COUNTROWS(
        FILTER(
            fact_nps_response,
            fact_nps_response[score] <= 6
        )
    )
VAR TotalResponses =
    COUNTROWS(fact_nps_response)

RETURN
IF(
    TotalResponses > 0,
    ( ( Promoters - Detractors ) / TotalResponses ) * 100
)
```

### Ventas Totales 
```dax
Total Ventas = 
CALCULATE(
    SUM(fact_sales_order[total_amount]),
    fact_sales_order[status_order] IN {"PAID","FULFILLED"}
)
```

### Usuarios Activos
```dax
Usuarios Activos = 
DISTINCTCOUNT(fact_web_session[customer_id])
```